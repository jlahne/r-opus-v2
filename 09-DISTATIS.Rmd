---
output: html_document
---

# DISTATIS

DISTATIS is the "discriminant sorting" version of STATIS, which according to @abdiSTATIS2012[p.125] is a French acronym for

> "Structuration des Tableaux Ã  Trois Indices de la Statistique" (which can, approximately, be translated as "structuring three way statistical tables").

Whereas STATIS is a methodology that is closely related to Multiple Factor Analysis [MFA: @abdiMultiple2013], DISTATIS is a slght elaboration of the method primarily for analyzing the results of **sorting tasks**.  In a sorting task, subjects receive a set of samples all at the same time (simultaneous sample presentation) and are asked to sort them into groups.  Typically, subjects are not given specific criteria for sorting, although variations of the method do exist that specify this.  Also typically, with $n$ samples, subjects are told to make between $2$ and $n-1$ groups (i.e., they cannot say all samples are identical or all samples are different).  With this simple set of instructions, a set of 15-25 subjects can evaluate a set of samples in a relatively efficient fashion.  The results are often not as well-defined or discriminating as traditional profiling methods (Descriptive Analysis, for the most part), but as a "first look" into the structure of a sample set, sorting tasks are often quite useful.

Two further points are worth mentioning: 

1. DISTATIS can be used on any dissimilarity data, not just that from sorting tasks.  It also does not require that the dissimilarity data be *binary* (as it is in a sorting task: in a group or not in a group).  For example, DISTATIS can be used with the results of hierarchical or multiple sorting [@courcouxFree2023].  Recently, "free linking" tasks, in which pairwise-similarity is judged among a set of samples (as opposed to group-wise similarity), have also been analyzed successfully with DISTATIS: these results are also non-binary similarities.
2. DISTATIS is closely related to MDS: it is an eigendecomposition on a set of dissimilarities.  It differs primarily in data-preprocessing and in the generation of additional analytical outputs, which are often useful.  The biplots produced by MDS and DISTATIS will usually be almost identical (see below).

With that said, let's launch into our application of DISTATIS.

## The dataset: sorting wines by color

We're actually going to use another dataset for this analysis, not the results of the DA we've been analyzing so far.  We're going to load the `sorting_r1` dataset, which according to HGH is 

> color data that came from sorting the wines into similarly colored groups.

Thus, the data pertain to the same wine, but is sorted into groups *by appearance* by 15 panelists.

```{r message=FALSE}
library(tidyverse)
library(here)

sorting_data <- read_csv(here("data/sorting_r1.csv"))

sorting_data
```

It looks like the data are presented as follows: each row corresponds to 1 of the 8 wine samples we've been dealing with so far, and each column represents a panelist (coded numerically), with the cells showing which group they assigned the wine to.  So, for example, panelist `263` assigned `I_REFOSCO` to group `G6`, whereas panelist `1331` assigned the same wine to group `G3`.  You'll notice, of course, that the problem with these data are that group numbers are dependent on panelist: it doesn't matter what actual number is assigned, but rather *what other wines each panelist thinks are in the same group*.  We need some way to show this data: this is exactly transforming groups into dissimilarities.

We are going to use the `DistatisR` package for much of the analysis in this section.  It has a utility function, `DistanceFromSort()`, that will transform this kind of grouping information into dissimilarities: symmetrical matrices for each panelist that show dissimilarity.  Let's check it out:

```{r message=FALSE}
library(DistatisR)

sorting_dissimilarities <- 
  sorting_data %>%
  # As is often the case, we need to move our ID column to `rownames`
  column_to_rownames("wine") %>%
  DistanceFromSort()

# Let's take a look at what we have now
str(sorting_dissimilarities)

# And here is a "slice" of the sorting "brick" (3D array or tensor)
sorting_dissimilarities[, , 1]
```

In order to deal with these data, the `DistanceFromSort()` function produces a "brick" of data (more typically called an "array" or a "tensor" in the machine-learning literature).  An array is a stack of matrices: the 3rd dimension, in this case, indexes subjects, so that `sorting_dissimilarities[, , 1]` gets the dissimilarity judgments of the first subject.  

These matrices are symmetric and contain only `0` and `1`: `0` when the row and column wines are in the same group, and `1` when they are not.  @abdiAnalyzing2007 have a demonstration that this in fact constitutes a Euclidean distance.

For subject #1, it looks like most wines were different, with only one group of 2 wines being similar by color (`C_MERLOT` and `C_ZINFANDEL`).

You'll notice that this process is much easier than the one used in the original **R Opus**.  The `DistatisR` package has matured (it incorporates the code file that HGH references) and so much of the preprocessing is now fully automatic.

## DISTATIS demonstrated

I am not going to go into all of the steps of DISTATIS [see instead @abdiAnalyzing2007], but briefly: DISTATIS first uses multivariate statistics to evaluate how similar all the subjects' sorts are to each other, then weights each subject by their similarity to the overall consensus, uses these weights to sum up the individual results into a consensus dissimilarity, and finally uses an eigendecomposition on the weighted sum to produce a map of (dis)similarities (this final step is the MDS-like part).  

This multistep approach means that not only do we have the consensus results, but we have access to results that tell us how much consensus there actually is, as well as the ability to generate bootstrapped confidence intervals based on individuals.

Luckily, the `DistatisR` package makes it quite easy to run DISTATIS.

```{r, text=FALSE}
distatis_results <- distatis(sorting_dissimilarities)

# We will also generate some bootstrapped results while we're at it so as to be
# able to plot confidence ellipses.
distatis_boots <- BootFromCompromise(sorting_dissimilarities)
```

Before we proceed, let's look at what we've done:

```{r}
distatis_results
```

You'll notice that the output from `distatis()` is a complex list object that is reasonably well-annotated (similar to what can be found in the `FactoMineR` package outputs).  We'll dig into this in a minute.

```{r}
distatis_boots %>% str

distatis_boots[, , 1]
```

The `BootFromCompromise()` function gives us another array: in this case a set of factor scores for the samples on the first 3 factors/components, over 1000 (the default) bootstrapped iterations.  Each slice is a single bootstrapped iteration.

Finally, let's examine our results.  The output from `distatis()` stores results about the *samples* in the `$res4Splus` list, and results about the judges in the `$res4Cmat` list.  These are named after the matrices defined in @abdiAnalyzing2007: the $\mathbf{C}$ matrix is the matrix of $RV$ coefficients between judges (a measure of multidimensional similarity), and the $\mathbf{S_+}$ matrix contains the compromise matrix of dissimilarities (after double-centering, weighting as I described above, and summing).  

Let's follow along in the order that HGH did in her workflow.  She starts by examining first the overall explained variances.  These are found in the `distatis_results$res4Splus$tau` vector.  

```{r message = FALSE}
distatis_results$res4Splus$tau %>%
  as_tibble(rownames = "dim") %>%
  mutate(dim = str_c("Dimension ", dim)) %>%
  ggplot(aes(x = dim, y = value)) + 
  geom_col(fill = "grey", color = "black", size = 1/2) + 
  theme_classic() + 
  labs(x = NULL, y = "% variance explained")
```

These results look the same as HGH's (good news!), and we can see our first two dimensions explain about 41% of the total variation in the sorts; this is fairly common for sorting, in which getting very high explanation in 2 dimensions is unlikely.

Let's look at how the products project into those first 2 dimensions.

```{r}
p_splus <- 
  distatis_results$res4Splus$F %>%
  as_tibble(rownames = "sample") %>%
  ggplot(aes(x = `Factor 1`, y = `Factor 2`)) + 
  geom_vline(xintercept = 0, color = "darkgrey") + 
  geom_hline(yintercept = 0, color = "darkgrey") +
  geom_point() + 
  ggrepel::geom_text_repel(aes(label = sample)) +
  coord_equal() + 
  theme_bw() + 
  labs(subtitle = "DISTATIS plot for compromise dissimilarities")

p_splus
```

These results are the same as HGH's, again, which is good news.  

HGH plotted the individual subjects as projected into this space.  It's a little annoying to do, and I do not think it is typically particularly illuminating, but we'll do it here for practice.  

```{r, message = FALSE}

p_splus_augmented <- p_splus
for(i in 1:dim(distatis_results$res4Splus$PartialF)[3]){
  augmented_data <- 
    distatis_results$res4Splus$PartialF[, , i] %>%
    as_tibble(rownames = "sample") %>%
    # and then add the compromise barycenters so we can draw lines to them
    bind_cols(
      distatis_results$res4Splus$F %>%
        as_tibble() %>%
        rename(xend = 1, yend = 2)
    )
  
  p_splus_augmented <- 
    p_splus_augmented +
    geom_point(data = augmented_data, 
               shape = 23) + 
    geom_segment(data = augmented_data,
                 aes(x = `Factor 1`, y = `Factor 2`,
                     xend = xend, yend = yend),
                 size = 1/4)
}

p_splus_augmented + 
  labs(title = "DISTATIS compromise plot",
       subtitle = "Individual subjects are projected as hollow diamonds")
```

We can see how each wine's position is actually the (weighted) barycenter of all the subject's judgments about where that wine should be grouped.  We could make this more easily readable by doing some work with colors and shapes, but I rarely find this kind of plot useful or informative.  I leave this to the reader!

Before we move on to examining the subjects and comparing to MDS, let's take a look at the bootstrapped confidence ellipses we generated.

```{r}
# We will need to reshape these arrays into a tibble that we can more easily
# feed into `stat_ellipse()`

# I hate converting arrays to tibbles - if anyone has a better way than a loop,
# let me know
boot_tbl <- tibble()

for(i in 1:1000){
  boot_tbl <- 
    bind_rows(boot_tbl,
              distatis_boots[, , i] %>%
                as_tibble(rownames = "sample"))
}

p_splus_boots <- 
  distatis_results$res4Splus$F %>%
  as_tibble(rownames = "sample") %>%
  ggplot(aes(x = `Factor 1`, y = `Factor 2`)) + 
  geom_vline(xintercept = 0, color = "darkgrey") + 
  geom_hline(yintercept = 0, color = "darkgrey") +
  stat_ellipse(data = boot_tbl,
               aes(fill = sample), 
               show.legend = FALSE, 
               geom = "polygon",
               alpha = 1/4) +
  geom_point(aes(color = sample), show.legend = FALSE) + 
  ggrepel::geom_text_repel(aes(label = sample)) +
  coord_equal() + 
  theme_bw() + 
  labs(subtitle = "DISTATIS compromise plot with bootstrapped ellipses") +
  scale_color_viridis_d() + 
  scale_fill_viridis_d()

p_splus_boots

```

With bootstrapping, we can see that there are 4 distinct groups of samples that don't appear to overlap even when we perturb the original data.

We can now turn our consideration to the subjects: how much did they agree?  We use the $RV$ matrix to examine this.

```{r}
distatis_results$res4Cmat$C %>%
  round(3)
```

$RV$ is a generalization of the concept of (Pearson) correlation to multivariate data.  $RV$ coefficients between two matrices can only be positive, and is constrained to the $[0,1]$ interval, with higher values indicating more agreement.  

If we use eigendecomposition on the $RV$ matrix, the coordinates of each subject on the first dimension indicate their maximum agreement.  We can use the first 2 dimensions to examine whether there are groups of subjects, as well as how well they agree in general:

```{r}
p_cmat <-
  distatis_results$res4Cmat$G %>%
  as_tibble(rownames = "subject") %>%
  ggplot(aes(x = `dim 1`, y = `dim 2`)) + 
  geom_vline(xintercept = 0, color = "darkgrey") + 
  geom_hline(yintercept = 0, color = "darkgrey") + 
  geom_point() + 
  ggrepel::geom_text_repel(aes(label = subject)) + 
  coord_equal() +
  theme_classic() +
  labs(subtitle = "DISTATIS subjects plot")
  
p_cmat
```

Subjects' positions on the x-axis here are proportional to the weight their sorts receive in the final consensus plot--we can see that overall subjects largely agree, with weights between just less than `0.5` up to about `0.85`.  On the second dimension we do see a fair amount of spread, so it is obvious that there is some disagreement among subjects.  However, without some grouping variable (like level of experience in winemaking), we can't interpret whether this disagreement is systematic.

## Comparison to MDS

HGH goes on to also analyze these data via metric and non-metric MDS.  We will use metric MDS for convenience.  Because MDS requires a distance matrix, we have to convert our original sorting data into some kind of consensus.  HGH used the `cluster::daisy()` function.  Reading the `?daisy` file, it appears that this is a general dissimilarity tool.  We'll compare it to simply summing up the observed groups.

```{r, warning = FALSE, fig.width = 8}

daisy_data <- 
  sorting_data %>% 
  column_to_rownames("wine") %>%
  # `daisy()` does not seem to accept string data
  mutate(across(everything(), ~as.factor(.))) %>%
  cluster::daisy()

simple_data <- 
  sorting_data %>%
  column_to_rownames("wine") %>%
  DistanceFromSort() %>%
  # Here we summ across the 3rd dimension of the array, "squishing" it 
  apply(c(1, 2), sum) %>%
  dist(method = "max")

p_mds_daisy <- 
  daisy_data %>%
  cmdscale() %>%
  as_tibble(rownames = "wine") %>%
  ggplot(aes(x = V1, y = V2)) + 
  geom_point() + 
  ggrepel::geom_text_repel(aes(label = wine)) + 
  coord_fixed() + 
  labs(x = "Dimension 1", y = "Dimension 2",
       subtitle = "MDS with dissimilarities derived from Gower's distance") + 
  theme_bw()

p_mds_simple <- 
  simple_data %>%
  cmdscale() %>%
  as_tibble(rownames = "wine") %>%
  ggplot(aes(x = V1, y = V2)) + 
  geom_point() + 
  ggrepel::geom_text_repel(aes(label = wine)) + 
  coord_fixed() + 
  labs(x = "Dimension 1", y = "Dimension 2",
       subtitle = "MDS with dissimilarities from simply summing sorts") + 
  theme_bw()

library(patchwork)

p_mds_simple + p_mds_daisy
```

We can see that the dissimilarities produced from `daisy()` and from our simple sum of dissimilarities are the same except for a scale constant.

Finally, we can compare this alignment to that from DISTATIS:

```{r, fig.width = 8}
p_splus + p_mds_simple
```

We can see that overall our results are extremely similar, although there are some changes (the incerased spread between the Merlot wines in the DISTATIS plot, and the decreased spread between the Refosco wines).  

## Packages used in this chapter

```{r}
sessionInfo()
```