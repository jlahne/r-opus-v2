---
output: html_document
---

# Generalized Procrustes Analysis

Generalized Procrustes Analysis (GPA) is the second-to-last topic HGH tackles in the original **R Opus**.  As a spoiler, this will be our last (substantive) topic: the coverage that HGH gave to Conjoint Analysis was cursory, and I believe that analysis of these data has moved very far forward since the original **R Opus**.  I don't ever use Conjoint Analysis (I leave that to my consumer-economist friends), and so I will not be including this chapter.

GPA is a "multi-table" method like MFA or DISTATIS.  GPA involves a set of (I believe) linear transformations of each data table through translation, rotation, and isotropic scaling, with the goal to minimize the difference between the original tables and the consensus table.  GPA was an early and popular method of aligning data from multiple subjects on the same sets of samples when those subjects are not trained.  Based on my understanding, it has fallen somewhat out of favor as MFA anecdotally and actually can provide very similar results [@tomicComparison2015].  This can be seen in the less-updated GPA function in the `FactoMineR` function, and by the fact that HGH does not bother to give it much time or interpretation.  GPA tends to be applied to (Ultra) Flash Profiling and Napping/Projective Mapping results, and it seems to me that those methods are less popular than they once were.  

However, the same authors point out several advantages of GPA, and so it is worth digging into it a little bit!  We're going to be using the same two-table dataset: DA and consumer data.

```{r setup-13, include=FALSE}
# save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# set a new output hook to truncate text output
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x <- xfun::split_lines(x)
    if (length(x) > n) {
      # truncate the output
      x <- c(head(x, n), "....\n")
    }
    x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})
```

```{r, message = FALSE}
library(tidyverse)
library(here)
library(FactoMineR)
library(paletteer)

# This is all just standard set up

descriptive_data <- 
  read_csv(here("data/torriDAFinal.csv")) %>%
  mutate(across(1:3, ~as.factor(.)))

consumer_data <- read_csv(here("data/torriconsFinal.csv"))

consumer_demo <- 
  consumer_data %>%
  select(Judge:Age) %>%
  mutate(across(Judge:Age, ~as.factor(.)))

consumer_data <- 
  consumer_data %>%
  select(-(`Wine Frequency`:Age)) %>%
  mutate(Judge = as.factor(Judge))
```

We then follow steps that will seem very familiar from MFA:

```{r, warning = FALSE}
preference_gpa <- 
  descriptive_data %>%
  group_by(ProductName) %>%
  summarize(across(where(is.numeric), mean)) %>%
  left_join(
    consumer_data %>%
      pivot_longer(-Judge, names_to = "wine") %>%
      pivot_wider(names_from = Judge, values_from = value),
    by = c("ProductName" = "wine")
  ) %>%
  column_to_rownames("ProductName") %>%
  GPA(group = c(20, 106), graph = FALSE)
```

I initially was confused about how it is possible to run GPA with two tables with such different number of measurements: in methods like STATIS we get around the difficulty by in fact analyzing the $\mathbf{X_iX_i^T}$, which will have the same dimensionality because each $\mathbf{X_i}$ has the same number of rows, even if the number of colums is different.  According to @gowerGeneralized1975, GPA solves this problem by instead just adding null columns to the column-wise smaller $\mathbf{X_i}$.

I believe that the GPA alignment is followed by a PCA on the consensus positions, since our points are still in 8-dimensional space ($min(n, k_1, k_2) = 8$ for our data with $n=8$ wines and $k_1=20$ wines and $k_2=106$ consumers), but this isn't explicitly clarified by the `?GPA` documentation--let's assume so!

Typically we haven't even wanted to *look* at base-`R` plots, but it is worthwhile to note that we can tell `GPA()` is less loved, because the function still outputs these, instead of the `factoextra`-flavored plots that other `FactoMineR` functions like `MFA()` and `PCA()` give us:

```{r}
plot(preference_gpa)
```

We know how to do better, though!

```{r}
# The `$consensus` table gives us the center points
p_gpa_base <- 
  preference_gpa$consensus %>%
  as_tibble(rownames = "wine") %>%
  ggplot(aes(x = V1, y = V2)) + 
  geom_vline(xintercept = 0, linewidth = 1/10) + 
  geom_hline(yintercept = 0, linewidth = 1/10) + 
  geom_point(aes(color = wine)) + 
  ggrepel::geom_text_repel(aes(label = wine, color = wine)) + 
  coord_equal() + 
  scale_color_paletteer_d("RSkittleBrewer::smarties") + 
  theme_bw() + 
  theme(legend.position = "none") +
  labs(subtitle = "GPA consensus for DA and consumer data",
       x = "Dimension 1",
       y = "Dimension 2")

p_gpa_base
```

We can then add layers showing the projected position of our two original tables:

```{r}
p_gpa_partials <-
  tibble(type = c("consensus", "DA", "consumer"),
       coord = list(preference_gpa$consensus,
                    preference_gpa$Xfin[, , 1],
                    preference_gpa$Xfin[, , 2])) %>%
  # Get everything into a tibble
  mutate(coord = map(coord, as_tibble, rownames = "wine")) %>%
  # Give every dimension the same name, since `GPA()` doesn't do this right
  mutate(coord = map(coord, set_names, c("wine", paste0("dim_", 1:7)))) %>%
  unnest(everything()) %>%
  # We're just going to do the first 2 dimensions here
  select(type:dim_2) %>%
  pivot_longer(dim_1:dim_2) %>%
  unite(type, name, col = "dim") %>%
  pivot_wider(names_from = dim, values_from = value) %>%
  # Plot!
  ggplot(aes(x = consensus_dim_1, y = consensus_dim_2)) + 
  geom_vline(xintercept = 0, linewidth = 1/10) + 
  geom_hline(yintercept = 0, linewidth = 1/10) + 
  # The projected DA data
  geom_point(aes(x = DA_dim_1, y = DA_dim_2),
             inherit.aes = FALSE, color = "darkgreen") + 
  geom_segment(aes(x = DA_dim_1, y = DA_dim_2, xend = consensus_dim_1, yend = consensus_dim_2),
               inherit.aes = FALSE, color = "darkgreen", linetype = 2) + 
  # The projected consumer data
  geom_point(aes(x = consumer_dim_1, y = consumer_dim_2),
             inherit.aes = FALSE, color = "tan") + 
  geom_segment(aes(x = consumer_dim_1, y = consumer_dim_2, xend = consensus_dim_1, yend = consensus_dim_2),
               inherit.aes = FALSE, color = "tan", linetype = 3) + 
  geom_point(aes(color = wine), size = 2) +
  ggrepel::geom_text_repel(aes(label = wine, color = wine)) + 
  coord_equal() + 
  scale_color_paletteer_d("RSkittleBrewer::smarties") +
  theme_bw() + 
  theme(legend.position = "none") +
  labs(subtitle = "GPA consensus for DA and consumer data with\npartial configurations",
       x = "Dimension 1",
       y = "Dimension 2",
       caption = "Green points represent DA data, tan points represent consumer data")

p_gpa_partials
```


We can compare this to our MFA map for reference (and to see the similarity between the methods.)

```{r mfa-plot, include = FALSE}
preference_mfa <- 
  descriptive_data %>%
  group_by(ProductName) %>%
  summarize(across(where(is.numeric), mean)) %>%
  left_join(
    consumer_data %>%
      pivot_longer(-Judge, names_to = "wine") %>%
      pivot_wider(names_from = Judge, values_from = value),
    by = c("ProductName" = "wine")
  ) %>%
  column_to_rownames("ProductName") %>%
  MFA(group = c(20, 106), graph = FALSE)


p_mfa <- 
  left_join(
    preference_mfa$ind$coord %>%
      as_tibble(rownames = "product"),
    preference_mfa$ind$coord.partiel %>%
      as_tibble(rownames = "product") %>%
      separate(product, into = c("product", "group"), sep = "\\.") %>%
      pivot_longer(-c(product, group)) %>%
      pivot_wider(names_from = c(name, group), values_from = value)
  ) %>%
  ggplot(aes(x = Dim.1, y = Dim.2)) +
  geom_vline(xintercept = 0, linewidth = 1/10) + 
  geom_hline(yintercept = 0, linewidth = 1/10) + 
  geom_point(aes(x = Dim.1_Gr1, y = Dim.2_Gr1), 
             color = "darkgreen", size = 1) + 
  geom_point(aes(x = Dim.1_Gr2, y = Dim.2_Gr2), 
             color = "tan", size = 1) + 
  geom_segment(aes(xend = Dim.1_Gr1, yend = Dim.2_Gr1), 
               color = "darkgreen", alpha = 1/2) + 
  geom_segment(aes(xend = Dim.1_Gr2, yend = Dim.2_Gr2),
               color = "tan", alpha = 1/2) + 
  geom_point(aes(color = product), 
             size = 2, show.legend = FALSE) + 
  ggrepel::geom_text_repel(aes(color = product, label = product), show.legend = FALSE) + 
  coord_equal() +
  theme_classic() + 
  theme(legend.position = "bottom") + 
  labs(x = paste0("Dimension 1, ", round(preference_mfa$eig[1, 2], 2), "% variance"),
       y = paste0("Dimension 2, ", round(preference_mfa$eig[2, 2], 2), "% variance"),
       subtitle = "MFA consensus map for DA and preference data\nwith projected individual table scores",
       caption = "DA scores are plotted in green, Consumer scores are plotted in tan.") + 
  scale_color_paletteer_d("RSkittleBrewer::smarties")
```

```{r, fig.width = 8}
library(patchwork)

(p_gpa_partials + p_mfa) & 
  labs(caption = NULL) &
  theme_classic() &
  theme(legend.position = "none")
```

We can see that we get close but not exactly similar configurations!

Returning to our GPA solution, we can inspect how close our configurations are, using both $RV$ and Procrustean similarity coefficients:

```{r}
preference_gpa$RV

preference_gpa$simi
```

We've already encountered $RV$, but to be honest I am not entirely sure what the definition of Procrustes similarity is.  A quick search doesn't really get me much more detail, nor does reviewing @gowerGeneralized1975.  However, from @qannariPerformance1998 I was able to find that Procrustes similarity is defined as 

$$s(X,Y)=\frac{trace(\mathbf{X^TYH})}{\sqrt{trace(\mathbf{X^TX})}\sqrt{trace(\mathbf{Y^TY})}}$$

This is clearly very close to the definition of the $RV$ coefficient, differing mainly in that the numerator is the Procrustes rotation that adjusts $\mathbf{Y}$ to $\mathbf{X}$, whereas for $RV$ the numerator instead represents the two double-centered cross-product matrices.  I am not sure what the Procrustes similarity emphasizes over the $RV$, but I believe it is more evenly influenced by dimensions in the data beyond the first principal component based on discussion in @tomicComparison2015.  So which should we pay attention to?  I don't have a good answer to that--it probably depends on the situation.

## Packages used in this chapter

```{r}
sessionInfo()
```