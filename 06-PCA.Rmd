---
output: html_document
---

# Principal Components Analysis (PCA)

While HGH is a strong proponent of Canonical Variate Analysis (CVA), it is hard to argue that Principal Components Analysis is not the "workhorse" tool for analysis of multivariate sensory datasets.  In fact, PCA is probably the most common tool for multivariate analysis, hard stop.  This is because it is so closely related to eigendecomposition and Singular Value Decomposition (SVD).  

We'll start with setting up our data as usual.  This time, we'll use the `FactoMineR` package for PCA because of its comprehensive outputs.  You could also use the base `R` function `princomp()`, but it provides fewer sensory-specific options.

```{r, message = FALSE}
library(tidyverse)
library(FactoMineR) # this is new
library(here)

descriptive_data <- 
  read_csv(here("data/torriDAFinal.csv")) %>%
  # note the use of across() to mutate multiple columns
  mutate(across(.cols = 1:3, ~as.factor(.)))
```

## What does PCA do?

As a reminder, in a dataset with multiple variables measured on the same observations, PCA finds *linear combinations* of the original variables that maximally explain the covariance among the original variables.  Thus, the criterion maximized by PCA is explanation of (co)variance.  We often describe this as "amount of variation explained" by the subsequent, new linear combinations.  In highly correlated data, which is usually the case in sensory data, PCA is very effective at finding a few new linear combinations of the original variables that explain most of the observed variance.  

Typically, PCA is conducted on mean vectors.

Speaking of CVA, recall that CVA finds linear combinations of variables that best explain *mean-vector* separation in the original data.  Thus, while CVA operates on raw data, it is looking to separate the mean-vectors or "barycenters" of the data.  PCA "knows" less information about the data: in fact, PCA is an "unsupervised" learning approach, in comparison to the "supervision" provided by the group IDs in CVA.  PCA is also generally lax about assumptions: the main assumption is that the observed variables are continuous.  If they are categorical, it is probably more appropriate to use (multiple) Correspondence Analysis.

PCA is equivalent to the eigendecomposition of the covariance matrix of our original variables.  If we standardize our variables (convert them to *z*-scores, or equivalently center our variables and standardize them to have unit variance) we will be conducting eigendecomposition on a correlation matrix.  Thus, you will often see discussion of "correlation" vs "covariance" PCA.  There are times in which one or the other is most appropriate for sensory results, but here we'll focus on correlation PCA.

A final note on PCA is that, because of the nature of eigendecomposition, the components are mutually orthogonal: they are uncorrelated and, geometrically, form right angles in the multivariate space.

For more details on PCA, I strongly recommend Herv√© Abdi's excellent and detailed chapter on the topic, [available on his website (I think A77 is the entry in his pub list)](https://personal.utdallas.edu/~herve/).

## Let's do PCA!

HGH starts the original **R Opus** with a function to take column and group means of the data in `descriptive_data` that she calls `mtable()`, for "means table".  Luckily for us, this functionality is a basic part of the `tidyverse`, and we've already used this approach in previous sections.  So let's generate a means table using `tidyverse`:

```{r}
descriptive_means <-
  descriptive_data %>%
  # What defines the group for which we want means?
  group_by(ProductName) %>%
  # Then we take the group means for every numeric variable (ignore NR, NJ)
  # Note the use of where() with across() to use a "predicate" function
  summarize(across(where(is.numeric), ~mean(.)))

descriptive_means
```

The `FactoMineR::PCA()` function is great, but it also tries to do way too much.  One of its annoying habits is a desire to give you a lot of plots you don't want.  So be sure to use the `graph = FALSE` argument so you have more control over plotting.  It also uses an older standard which relies on storing observation IDs in `row.names`--this isn't great programming practice and we have to explicitly do so.  Following HGH, we are going to conduct a covariance PCA by setting `scale.unit = FALSE`.

```{r}
means_pca <- 
  descriptive_means %>%
  column_to_rownames("ProductName") %>%
  PCA(scale.unit = FALSE, graph = FALSE)

means_pca
```

The nice thing about `PCA()` is that it gives a well-structured list of results that we can do a lot with.  First off, let's make a quick "scree plot", describing the variance explained by each of the principal components.

```{r}
# Here are the actual numeric results.
means_pca$eig

# And now we can plot
means_pca$eig %>%
  # Note that we need to use `rownames=` to capture the rownames from PCA()
  as_tibble(rownames = "component") %>%
  ggplot(aes(x = component, y = `percentage of variance`, group = 1)) + 
  geom_line() +
  theme_bw()
```

We can see that the "elbow" here occurs at the third or fourth component, but we're going to only examine the first 2 components.  Keep in mind that this means some variation might not be apparent.

The so-called "loadings" in a PCA are the weights of the linear combination of original variables that make up each component.  We access them in the `$var$coord` table in the results from `PCA()`.

```{r}
means_pca$var$coord
```

Technically, the number of dimensions we *could* get from a PCA is equal to the $min(n-1, k)$, where $n$ is the number of product means we have and $k$ is the number of measured variables, but in practice we won't typically examine more than 3-4 components/dimensions, as having to examine more for our purposes would indicate that PCA may not be the right tool for dimension reduction.

```{r}
p_loadings <- 
  means_pca$var$coord %>%
  as_tibble(rownames = "descriptor") %>%
  ggplot(aes(x = Dim.1, y = Dim.2)) + 
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 1/4) +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = 1/4) +
  geom_segment(aes(xend = 0, yend = 0), 
               arrow = arrow(length = unit(0.1, "in"), ends = "first")) +
  ggrepel::geom_text_repel(aes(label = descriptor)) + 
  theme_bw() +
  coord_fixed() + 
  labs(title = "Loadings plot for PCA of product means",
       x = paste0("Dimension 1 (", round(means_pca$eig[1, 2], 2), "%)"),
       y = paste0("Dimension 2 (", round(means_pca$eig[2, 2], 2), "%)")) 

p_loadings
```

Typically, we'd interpret this by looking at variables that are very "close" (make acute angles to) the x- or y-axes (which are the first and second components, respectively), and that are very long.  The magnitude of the vector indicates the coefficient for that variable in the linear combination making up each of the two principal components that serve as axes of the displayed space.  Therefore, we'd say that the fruity flavors (e.g., `Jam`) are loading strongly and positively on the first component, and almost not at all to the second component, and that the strongest negative contributions come from `Burned`, which is also strongly negatively loaded on the second component.  For the second component, `Medicinal` and `Band-aid`, with other associated flavors, are strongly positively loaded (and negatively loaded on the first dimension), whereas `Sour` loads positively almost entirely on the first component, but not as strongly.  There are many other observations we could make from this plot.  It is worth comparing it to the loading/coefficient plot for the CVA; you will see that while the values of the coefficients are not identical, the patterns are very similar.

As a side note (because I had to puzzle this out myself), the `$var$coord` matrix is *not* the raw $\mathbf{Q}$ loadings matrix from singular value decomposition (SVD), which we'd use to find the scores for the original observations.  Rather, if we write the SVD as $\mathbf{X} = \mathbf{P \Delta Q}^T$, it is $\mathbf{\Delta Q}^T$.  In effect, the "coordinates" given in `$var$coord` are expanded by the size of the singular value for the associated principal component.  We can find the raw loadings (coefficient), $\mathbf Q$ in the `$svd$V` matrix in the results from the `PCA()` function.  

We find scores for our product means by using the linear combinations described by the loadings.  So, we could by hand calculate:

```{r}
means_pca$svd$V
```

This tells us that, to get a mean vector $i$'s score on principal component 1 (Dimension 1), we would calculate $PC_i = 0.24 * Red\_berry_i + ... -0.09 * Astringent_i$, and so on (note that because this is a raw matrix, it has no row names or descriptive columns; I am getting the loadings for specific descriptors by looking back at what the 1st and 20th descriptors are in our data set and matching them up manually).

Before we leave this topic, I want to point out one more thing: if we plot the raw loadings, we'll come to the same conclusions:

```{r}
means_pca$svd$V %>%
  as_tibble() %>%
  bind_cols(descriptor = row.names(means_pca$var$coord)) %>%
  rename(Dim.1 = V1, Dim.2 = V2) %>%
  ggplot(aes(x = Dim.1, y = Dim.2)) + 
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 1/4) +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = 1/4) +
  geom_segment(aes(xend = 0, yend = 0), 
               arrow = arrow(length = unit(0.1, "in"), ends = "first")) +
  ggrepel::geom_text_repel(aes(label = descriptor)) + 
  theme_bw() +
  coord_fixed() + 
  labs(title = "Raw (unscaled) loadings plot for PCA of product means",
       x = paste0("Dimension 1 (", round(means_pca$eig[1, 2], 2), "%)"),
       y = paste0("Dimension 2 (", round(means_pca$eig[2, 2], 2), "%)")) 
```


OK, with all that said, if we multiply our means-vector ratings (mean-centered for each column) by the loadings we just spent a while getting, we get the *scores* for our mean vectors in the PCA space.  These are stored in the `$ind$coord` matrix. 

```{r}
p_scores <- 
  means_pca$ind$coord %>%
  as_tibble(rownames = "product") %>%
  ggplot(aes(x = Dim.1, y = Dim.2)) + 
  geom_hline(yintercept = 0, linetype = "dashed", linewidth = 1/4) +
  geom_vline(xintercept = 0, linetype = "dashed", linewidth = 1/4) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = product)) + 
  theme_bw() +
  coord_fixed() + 
  labs(title = "Scores plot for PCA of product means",
       x = paste0("Dimension 1 (", round(means_pca$eig[1, 2], 2), "%)"),
       y = paste0("Dimension 2 (", round(means_pca$eig[2, 2], 2), "%)")) 

p_scores
```

We interpret this plot by noting the spatial separation of sample mean-vectors, as well as noting the proximity to the axes, which we interpret by their loadings from variables.  In order to facilitate this second task, it is often helpful to have the loadings and scores plots side by side.  We can accomplish this using the nifty `patchwork` package, which lets us arrange saved plots however we want.

```{r fig.width = 9}
library(patchwork)

p_scores + p_loadings + plot_layout(widths = c(2, 2))

```

By looking at these plots together, we can see that the first dimension, which we previously noted separated fruity flavors from medicinal and burnt flavors, is driven by a separation of the Italian Primitivo and Syrah samples from the other samples.  The second dimension, which is separating the medicinal and burnt flavors, is interestingly also separating the Californian and Italian wines made from two sets of the same grape, Refosco and Merlot.

## In-depth interpretation

There are several ways that we can get more information about the  structure of the PCA solution.  First, we will follow HGH and investigate the *correlations* between the variables and each of the first two components.  HGH used the `FactoMineR::dimdesc()` function, but we're going to do that later.

We can access the correlations directly using `$var$cor` in the output of the `PCA()` function.  I'll turn it into a tibble to make display easier.

```{r}
# The most highly correlated variables with Dimension 1
means_pca$var$cor %>%
  as_tibble(rownames = "descriptor") %>%
  select(1:3) %>%
  arrange(-Dim.1) %>%
  slice(1:3, 18:20)

# The most highly correlated variables with Dimension 2
means_pca$var$cor %>%
  as_tibble(rownames = "descriptor") %>%
  select(1:3) %>%
  arrange(-Dim.2) %>%
  slice(1:3, 18:20)
```

These are *also* called loadings [@abdiPrincipal2010].  The semantics of PCA are a nightmare.  We can see that these reflect our interpretation from the loadings plot pretty accurately.

HGH then calculated the "communalities" for variables on the first and second dimensions of the PCA solution.  I had to do some digging to figure out what she meant by this, but she described them as:

> Communality is the sum of the squared loadings for the number of dimensions that you would like to keep.

We know from @abdiPrincipal2010 that the sum of squared loadings (in the sense of *correlations*) for each dimension should sum to 1, so this allows us to speak of "proportion of explained variance" for each variable and each dimension.  But we have to remember the semantic overloadings here, so if we want to see this property we will need to look at the `$svd$V` matrix again.  The `$var$cor` matrix, remember, stores this scaled by the original variables (maybe this is instead storing covariances). We can see this pretty easily:

```{r}
# The "cor" matrix doesn't seem to really be storing what we think of as 
# correlations.
means_pca$var$cor %>%
  as_tibble(rownames = "descriptor") %>%
  pivot_longer(-descriptor) %>%
  mutate(squared_loading = value ^ 2) %>%
  group_by(name) %>%
  summarize(total = sum(squared_loading))

# Whereas the SVD list behaves as expected.
means_pca$svd$V %>%
  as_tibble() %>%
  pivot_longer(everything()) %>%
  mutate(value = value ^ 2) %>%
  group_by(name) %>%
  summarize(total = sum(value))
```


### Correlations or (squared) loadings or contributions

The correlations between the variables and the components are also unfortunately called "*loadings*" [cf. @abdiPrincipal2010 for more info], but these are distinct (although related, argh) from the "loadings" we discussed as the elements of the $\mathbf Q$ matrix from SVD.  

We could directly calculate these, but we can make use of the `dimdesc()` convenience function

```{r}
dimdesc(means_pca, axes = c(1, 2))
```

HGH calls these squared correlations the "communalities" of the variables, which should(?) sum to 1.  This is not the case here because we have a typical $n<p$ problem: there are more variables than observations (when we run a PCA on the means of the products across panelists and reps).  

There are a number of other ways to interpret the over-loaded "*loadings*"--the role of the variables in determining the new principal-components space in PCA results.  To return to the progression of the **R Opus**, let's follow HGH in examining the **Contributions** and the **Communalities** in the PCA results.

According to @abdiPrincipal2010[p.8-9],

> ...the importance of an observation for a component can be obtained by the ratio of the squared factor score of this observation by the eigenvalue associated with that component.

and

> The value of a contribution is between 0 and 1 and, for a given component, the sum of the contributions of all observations is equal to 1. The larger the value of the contribution, the more the observation contributes to the component. A useful heuristic is to base the interpretation of a component on the observations whose contribution is larger than the average contribution (i.e., observations whose contribution is larger than $1/I$).

While Abdi and Williams are talking about the *contribution* of an observation, since PCA is (largely) agnostic about the role of observation (product) and variable (descriptor), `FactoMineR::PCA()` will return contributions for both products and descriptors, found as usual in the `ind` and `var` sub-lists of the results.  We don't care so much about the products' contributions, in this case, but we do care about the variables'.  We can find and print them:

```{r}
means_pca$var$contrib %>% round(3)
```

Note that `FactoMineR` seems to scale the contributions to a percentage (e.g., multiply by 100), rather than returning contributions in the range $[0,1]$.  Following Abdi & Williams' suggestion above, we can do a little wrangling to see important contributions visually:

```{r}
# First we make a tibble
means_pca$var$contrib %>%
  as_tibble(rownames = "descriptor") %>%
  # Then we select the first 2 dimensions (for ease)
  select(descriptor, Dim.1, Dim.2) %>% 
  # For both plotting and filtering, long-type data will be easier to work with
  pivot_longer(-descriptor) %>%
  # We can now choose only contributions > 100 / # of descriptors (that is, 20)
  filter(value > 100 / 20) %>%
  # We use some convenience functions from `tidytext` to make our facets nicer
  mutate(descriptor = factor(descriptor) %>%
           tidytext::reorder_within(by = value, within = name)) %>%
  # And now we plot!
  ggplot(aes(x = descriptor, y = value)) + 
  geom_col(aes(fill = name), show.legend = FALSE) + 
  tidytext::scale_x_reordered(NULL) + 
  coord_flip() + 
  theme_bw() + 
  facet_wrap(~name, scales = "free") +
  labs(x = "Contribution (in %)")
```

We can see that for PC1, contributions seem to be coming from a lot of fruity flavors, as well as some influence from complex flavors that I would attribute to possible Brettanomyces influence in some of the wines.  In PC2, there appears to be instead more influence of oak ("Chocolate" and "Vanilla") as well as the same Brettanomyces flavors.  Note that contributions, as squared measurements, are always positive - these are *absolute* measures of influence on the dimensions.

HGH says in the original **R Opus** that

> Communality is the sum of the squared loadings for the number of dimensions that you would like to keep.

I am not sure I quite follow what she did, as she then goes on to examine the contributions, which as we've described are the squared loadings divided by the eigenvalues so that they sum to 1.  In @abdiPrincipal2010 they don't discuss *communality*, and if I remember properly the concept is more frequently applied to Factor Analysis [@rencherMethods2002], so we'll leave it for now.  I think that this is a great example of how closely overlapping concepts can get confusing in the world of components-based methods, since to my understanding Factor Analysis, in some of its simpler forms, can be derived directly from PCA but with different assumptions mapped onto the steps. 

## PCA with resampling for confidence intervals

In the original **R Opus**, HGH uses the `SensoMineR::panellipse()` function to generate confidence ellipses for the product mean vectors in PCA.  

```{r, results='hide'}
panellipse_res <- 
  # We have to reimport the data because panellipse() doesn't behave well with
  # tibble() formats.
  SensoMineR::panellipse(donnee = read.csv(here("data/torriDAFinal.csv")), 
                         col.p = 2, col.j = 1, firstvar = 4, scale.unit = FALSE)
```

I'm not a huge fan of `panellipse()` because it's pretty opaque.  I can't find the documentation on what it's doing, and there doesn't seem to be an associated journal article.  It doesn't really document how it is resampling or give easily understood descriptions of what the results (both numerical and graphical) it is producing *mean*.  Here is the plot that HGH uses for the original **R Opus**:

```{r}
panellipse_res$graph$plotIndEll
```

The confidence ellipses are definitely being drawn around 95% of the resampled results from the bootstrapping procedure, but I'm not sure if this is a bootstrap based on, for example, the "partial bootstrap" or the "truncated bootstrap".  We will us a naive approach to producing a partial bootstrap in order to do some resampling and compare it.

It is also worth noting that the plot produced here is different than that produced in the original **R Opus**, so either there is simulation variability (probably) or the underlying program has changed between 2015 and now (also possible).  The overall conclusions are not greatly different but the overlapping areas can vary quite dramatically.

The basic approach (which we saw back in the CVA section of the **R Opus**) is to draw a new set of bootstrapped observations for each product: we need 42 observations per product to calculate a new mean.  We then can use the projection function from our original PCA solution to project these results into our space; in a nod to the truncated bootstrap approach [@cadoretConstruction2013] we will use only the first 2 dimensions of the projection function to get the results so as not to overfit.  Finally, we'll draw ellipses around our results to represent variability.

```{r}
get_bootstrapped_pca_means <- function(){
  
  descriptive_data %>%
    select(-NJ, -NR) %>%
    group_by(ProductName) %>%
    # Here we resample each wine - we draw 42 new observations for each wine
    # with replacement
    slice_sample(prop = 1, replace = TRUE) %>%
    # We calculate the mean for the newly drawn samples
    summarize_if(is.numeric, ~mean(.)) %>%
    # And then we center the means (by subtracting the column means)
    mutate_if(is.numeric, ~. - mean(.)) %>%
    nest(data = -ProductName) %>%
    mutate(means = map(data, ~as.matrix(.x)),
           pc1 = map_dbl(means, ~ .x %*% means_pca$svd$V[, 1]),
           pc2 = map_dbl(means, ~ .x %*% means_pca$svd$V[, 2])) %>%
    select(-data, -means)
  
}

# Make it reproducible
set.seed(6)

pca_boots <- 
  tibble(boot_id = 1:100) %>%
  mutate(bootstrapped_data = map(boot_id, ~get_bootstrapped_pca_means())) %>%
  unnest(bootstrapped_data)

p_scores +
  geom_point(data = pca_boots, 
             inherit.aes = FALSE,
             aes(x = pc1, y = pc2, color = ProductName), 
             alpha = 1/2, size = 1/2) +
  stat_ellipse(data = pca_boots, inherit.aes = FALSE,
               aes(x = pc1, y = pc2, color = ProductName))
```

Our results are pretty close, but not exactly the same.  It seems like our method of generating bootstrapped scores (via resampling followed by projection via the $\mathbf Q$ matrix from SVD) is potentially more liberal in product separation than that from the `panellipse()` function.  Perhaps `panellipse()` is using the "truncated bootstrap" approach [@cadoretConstruction2013], which solves a full PCA with the resampled data, then aligns it with the original observed space via [Generalized Procrustes Analysis], then repeats that process a large number (e.g., 1000) times.. 

## Comparison of products with PCA

HGH then used the `panellipse()` results to get Hotelling's $T^2$ stats for each set of products.  I believe that Hotelling's $T^2$ is a generalization of the $t$-distribution to multivariate data.  These were accessed from the outputs of `panellipse()`, which we stored in `panellipse_res`.  

```{r}
names(panellipse_res)
```

The `SensoMineR::coltable()` function HGH used is a visualization function for this kind of output, let's take a look.

```{r}
SensoMineR::coltable(panellipse_res$hotelling, main.title = "Hotelling's T2 for all products")
```

Let's practice how to make a similar table from this kind of data.  The actual `panellipse_res$hotelling` object is just a square matrix.  We can use this as input for something like the `geom_tile()` function with the right kind of wrangling.  

```{r}
# First wrangle

panellipse_res$hotelling %>%
  as_tibble(rownames = "x") %>%
  pivot_longer(-x, names_to = "y") %>%
  mutate(color = if_else(value < 0.05, "pink", "white")) %>%
  
  # Now plot
  
  ggplot(aes(x = x, y = y)) +
  geom_tile(aes(fill = color),
            color = "black") +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_manual(values = c("pink", "white")) + 
  coord_fixed() + 
  
  # Everything after this is just making the plot look nice
  
  scale_x_discrete(position = "top") + 
  scale_y_discrete(limits = rev) + 
  labs(x = NULL, 
       y = NULL,
       title = bquote("Pairwise Hotelling's"~italic(T)^2),
       subtitle = bquote(italic(p)*"-values"<0.05~"are highlighted in pink")) + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 270, hjust = 1),
        legend.position = "none",
        axis.ticks = element_blank(),
        panel.grid = element_blank())
```

Notice the `ggplot2` syntax above is kind of complicated, but that's because I did it all at once, and I wanted to do a lot of minor things like remove axis ticks, so as to replicate the plot from  the `panellipse()` function closely.

As a bonus, we will quickly look into how to conduct Hotelling's $T^2$ tests ourselves, and then leave the world of PCA (for now) to turn to methods for cluster analysis.

```{r}
# We need pairs of products - if we wanted to make all pairwise comparisons it
# would be possible to do so using, for example, nested `for()` loops or some
# kind of list-table structure
hotelling_demo_data <- 
  descriptive_data %>%
  filter(ProductName %in% c("C_MERLOT", "C_REFOSCO")) %>%
  select(-NJ, -NR)

DescTools::HotellingsT2Test(formula = as.matrix(hotelling_demo_data[, -1]) ~ ProductName,
                            data = hotelling_demo_data)
```

These results are not the same as those given in the `panellipse()` output; I suspect after reading `?panellipse` that this is because internally that function is running a Hotelling's $T^2$ test on the PCA results, rather than on the raw data, but I am not sure and I am not willing to try to interpret the under-the-hood code.  If you know, please reach out and let me know!

In general, it seems like using something like CVA and investigating the separability of means there instead of using pairwise Hotelling's $T^2$ tests would be safer in terms of familywise error inflation.

## Packages used in this chapter

```{r}
sessionInfo()
```