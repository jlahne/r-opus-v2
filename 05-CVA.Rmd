---
output: html_document
---

# Canonical Variate Analysis

In the analysis of sensory data, many of the steps are concerned with "dimensionality reduction": finding optimal, low-dimensional representations of sensory data, which is typically highly multivariate.  The definition of "optimal" will vary, depending on the particular method chosen.  In this section, we're going to focus on **Canonical Variate Analysis (CVA)**, which HGH tends to prefer for sensory data over the more common Principal Components Analysis (we'll get to that, too!).

First, we'll start with loading our data and the relevant libraries.  Note that here we will be loading a new library: `candisc`.  This is the library that HGH uses in the original **R Opus**, and as far as I can tell it remains the best way to conduct CVA.  A good alternative would be Beaton & Abdi's `MExPosition`, but that package appears to be defunct as of the time of this writing (late 2024), and uses a closely related approach called Barycentric Discriminant Analysis instead of the typical CVA.

```{r, message = FALSE}
# Attempted fix for the rgl error documented here: https://stackoverflow.com/a/66127391/2554330

#options(rgl.useNULL = TRUE)
library(rgl)

library(tidyverse)
library(candisc) # this is new
library(here)

descriptive_data <- read_csv(here("data/torriDAFinal.csv")) %>%
  mutate_at(.vars = vars(1:3), ~as.factor(.))
```

## What is CVA?

Canonical Variate Analysis is also often called (Linear) Discriminant Analysis.  It is closely related to Canonical Correlation Analysis, and is even sometimes called Fisher's Linear Descriminant Analysis (phew).  The basic goal of this analysis is to find linear combinations of variables that best separate groups of observations.  In this way, we could say that CVA/LDA is a "supervised" learning method, in that we need to provide a vector of group IDs for the observations (rows) in order to allow the algorithm to find a combinaton of variables to separate them.   

CVA builds on the same matrix-math calculations that underly MANOVA, and so, according to @rencherMethods2002, we can think of CVA as the multivariate equivalent of univariate post-hoc testing, like Tukey's HSD.  In post-hoc testing, we calculate various intervals around group means in order to see if we can separate observations from multiple groups according to their group IDs.  We do much the same with CVA, except that we use group mean *vectors*.  

Recall that we had a significant 3-way MANOVA for our data:

```{r}
manova_res <- 
  manova(as.matrix(descriptive_data[, 4:23]) ~ (ProductName + NJ + NR)^2, 
         data = descriptive_data)

summary(manova_res, test = "W")
```

We are interested in building on our intuition about Mahalanobis distances, etc, and seeing if we can use, in our case, `ProductName` to separate our observations in multidimensional descriptor space.  Specifically, we have a 20-dimensional data set:

```{r}
descriptive_data %>%
  glimpse()
```

We can't see in 20 dimensions!  So how does CVA help us not only separate our observations, but visualize them?  It turns out, without digging too far into the matrix algebra, that the following are properties of CVA:

1.  For $k$ groups and $p$ variables, CVA will find $min(k-1, p-1)$ linear functions of the variables that best separate the groups.  
2.  The solution turns out to be an eigendecomposition of the MANOVA calculations.
3.  Because it is an eigendecomposition, we can rank the functions by their associated eigenvalues, and say that the function associated with the largest eigenvalue best separates the groups.
4.  We can treat the functions, which map observation vectors to single values using a linear combination of variables, as cartesian axes to plot variables against each other.
1.  **NB:** In fact, unlike PCA and some other approaches, CVA does not in general produce orthogonal linear functions, and so we are technically distorting the space a bit by plotting canonical variate function 1 as the x-axis, canonical variate function 2 as the y-axis, and so on, but in practice the distortion is reported to be minor [@rencherMethods2002, @heymannSensory2017].

OK, back outside of the math digression, let me resummarize: we can use CVA to find combinations of the original variables that best separate our observations into their assigned group IDs.  Because this is often the exact task we want to accomplish with a Descriptive Analysis, CVA is a great tool to use to describe variation in samples' descriptive profiles according to the important categorical variable(s) that identify them.

## Application of CVA

In the original **R Opus**, HGH recommends using CVA only on one-way MANOVA, and we ran a 3-way MANOVA.  This makes sense if you think in analogy to univariate post-hoc tests: these are sometimes called "marginal means" tests because we marginalize across the other factors to collapse our data to only a single categorical predictor in which we're interested.  However, we are able to run post-hoc tests on multi-way ANOVA, and we can do the same thing in MANOVA/CVA land.  Let's compare the approach for the two.

First, let's run a one-way MANOVA with just ProductName as the predictor and then send it off to CVA (using the `candisc()` function, whose name comes from "canonical discriminant analysis"):

```{r}
manova_res_oneway <- 
  manova(as.matrix(descriptive_data[, 4:23]) ~ ProductName, 
         data = descriptive_data)

cva_res_oneway <- candisc(manova_res_oneway)
cva_res_oneway
```

The likelihood-ratio tests given in the summary from the `cva_res_oneway` object gives tests derived from the *F*-statistic approximation to the Wilk's $\Lambda$ results from the original MANOVA (another reason we prefer to use that statistic).

However, compare these results to those from our original MANOVA:

```{r}
cva_res <- candisc(manova_res, term = "ProductName")
cva_res
```

While the exact numbers are different (and we see more significant dimensions according to the LR test), the actual amount of variation explained by the canonical variates is very close.  Furthermore, we can  visualize the results to contrast them.

### Basic visualization of CVA

We'll start by examining the placement of the group means in the resultant "CVA-space", which is just the scores (linear combinations of original variables) for the first and second canonical variate functions for each product mean vector.

```{r}
# Here is the plot of canonical variates 1 and 2 from the one-way approach
scores_p_1 <- 
  cva_res_oneway$means %>%
  as_tibble(rownames = "product") %>%
  ggplot(aes(x = Can1, y = Can2)) +
  geom_point() + 
  ggrepel::geom_text_repel(aes(label = product)) + 
  theme_bw() + 
  labs(x = paste0("Canonical Variate 1, (", round(cva_res_oneway$pct[1], 2), "%)"),
       y = paste0("Canonical Variate 2, (", round(cva_res_oneway$pct[2], 2), "%)"),
       title = "CVA Group Means (Scores) Plot",
       subtitle = "based on the effect of wines in a 1-way MANOVA") +
  coord_equal()

scores_p_1

# And here is the plot of the canonical variates 1 and 2 from the multiway MANOVA
scores_p_2 <- 
  cva_res$means %>%
  as_tibble(rownames = "product") %>%
  ggplot(aes(x = Can1, y = Can2)) +
  geom_point() + 
  ggrepel::geom_text_repel(aes(label = product)) + 
  theme_bw() + 
  labs(x = paste0("Canonical Variate 1, (", round(cva_res$pct[1], 2), "%)"),
       y = paste0("Canonical Variate 2, (", round(cva_res$pct[2], 2), "%)"),
       title = "CVA Group Means (Scores) Plot",
       subtitle = "based on the effect of wines from the multiway MANOVA") +
  coord_equal()

scores_p_2
```

Besides a reversal of the x-axis (a trivial reflection which does not change meaning), the product mean separation appears to be identical in these two solutions, and the proportion of explained variation remains almost identical.  We'll keep up this comparison as we investigate the rest of the solution.

Next, we want to examine how the original variables are related to the resulting "CVA-space".  This is usually visualized as a vector plot, so we're going to follow convention.  This will make our `ggplot2` calls a little messsier, but I think it is worth it.

In the `candisc()` output, the `$coeffs.raw` list holds the linear combinations of variables that contribute to each dimension:

```{r}
cva_res_oneway$coeffs.raw %>%
  round(2) %>%
  as_tibble(rownames = "descriptor")
```

This tell us that to get an observation's (call it $i$) score on Canonical Variate 1, we'd take the following linear sum: $Can1_i = -0.01*Red\_berry_i + -0.12*Dark\_berry_i + ... + -0.22*Astringent_i$.  We do this for every observation to find their score in the space.  

To visualize this, rather than plotting the raw coefficients, we typically plot the correlations between the raw coefficients and the original variables, which are stored in the `$structure` list.  We can consider the "loadings" or "structure coefficients" themselves as coordinates in the same CVA space, which gives us an idea of how each variable contributes to the dimensions.

```{r}
cva_res_oneway$structure %>%
  as_tibble(rownames = "descriptor") %>%
  ggplot(aes(x = Can1, y = Can2)) + 
  geom_segment(aes(xend = 0, yend = 0), 
               arrow = arrow(length = unit(0.2, units = "in"), ends = "first")) +
  ggrepel::geom_text_repel(aes(label = descriptor)) +
  labs(x = paste0("Canonical Variate 1, (", round(cva_res_oneway$pct[1], 2), "%)"),
       y = paste0("Canonical Variate 2, (", round(cva_res_oneway$pct[2], 2), "%)"),
       title = "CVA Loadings Plot",
       subtitle = "based on the variable loadings from the 1-way MANOVA") +
  theme_bw()
```

We can see from this plot that the first canonical variate has large positive contributions from "Burned" characteristics and smaller contributions from "Oak", and strong negative contributions from fruit-related terms like "Jam", "Artificial_frui [sic]", etc.  The second canonical variate has large positive contributions from both "Vanilla" and "Chocolate", and negative contributions from flaw/complex terms like "Medicinal", "Band-aid", etc.

If we examine the results from the multiway MANOVA we'll see similar results, with the exception of a reversed x-axis (first canonical variate).  The main differences seem to be better separation among the fruit-like contributors, with "Artificial_frui" and "Jam" being better separated, and an implication that the "Red_berry" and "Dark_berry" attributes are probably contributing more to a higher dimension.

```{r}
cva_res$structure %>%
  as_tibble(rownames = "descriptor") %>%
  ggplot(aes(x = Can1, y = Can2)) + 
  geom_segment(aes(xend = 0, yend = 0), 
               arrow = arrow(length = unit(0.2, units = "in"), ends = "first")) +
  ggrepel::geom_text_repel(aes(label = descriptor)) +
  labs(x = paste0("Canonical Variate 1, (", round(cva_res$pct[1], 2), "%)"),
       y = paste0("Canonical Variate 2, (", round(cva_res$pct[2], 2), "%)"),
       title = "CVA Loadings Plot",
       subtitle = "based on the variable loadings from the multiway MANOVA") +
  theme_bw()
```

### An aside on exporting R plots

In the original **R Opus**, HGH used RStudio's "Export" pane to get files from the `Plots` pane to a format that could be edited in PowerPoint.  I tend to discourage this approach, because it produces overall lower resolution results.  In addition, as you will have noticed I use `ggplot2` almost exclusively for visualizations, and the `ggplot2::ggsave()` function gives easy functional access to `R`'s somewhat byzantine `device` interface, making saving images directly much more feasible.

To use `ggsave()`, you can either save an executed `ggplot2` plot as an object and pass it to the function, or let the default `ggsave(plot = last_plot())` behavior grab the current visual you're looking at in the `Plots` pane.  Either way, you then need to specify a destination file *with extension* (e.g., `/path/to/your/image.png`), which `ggsave()` will parse to get the correct output format.  I tend to output as `.png`, but you might also want to output `.jpg` or `.tif`, depending on the usecase.  Use `?ggsave` for more details.

You will also need to specify arguments for `height = ` and `width = `, as well as a `units = `, which takes a character string indicating pixels, inches, centimeters, etc.  Finally, the `scale = ` argument is equivalent to the base `R` `cex = ` arguments: it dictates the degree to which the plot will be "zoomed".  `scale = ` values that are greather than 1 will cause the plot to "zoom out" (elements in the plot will appear smaller), and values less than 1 will cause a "zoom in" effect.

An example of this workflow, the following (not executed) code will take the last plot of the CVA loadings for the multiway ANOVA, save it as an object called `p`, and then save that plot to my home folder:

```{r, eval = FALSE}
p <-
  cva_res$structure %>%
  as_tibble(rownames = "descriptor") %>%
  ggplot(aes(x = Can1, y = Can2)) + 
  geom_segment(aes(xend = 0, yend = 0), 
               arrow = arrow(length = unit(0.2, units = "in"), ends = "first")) +
  ggrepel::geom_text_repel(aes(label = descriptor)) +
  labs(x = paste0("Canonical Variate 1, (", round(cva_res$pct[1], 2), "%)"),
       y = paste0("Canonical Variate 2, (", round(cva_res$pct[2], 2), "%)"),
       title = "CVA Loadings Plot",
       subtitle = "based on the variable loadings from the multiway MANOVA") +
  theme_bw()

ggsave(filename = "~/my_new_plot.png", 
       plot = p, height = 6, width = 8, 
       units = "in", scale = 1.2)
```

## Plotting uncertainty in CVA

One of the advantages of CVA is that it is based on raw data, which PCA is *typically* not [an exception is so-called Tucker-1 PCA, @dettmarPrincipal2020].  So far we've only visualized the *group* means for the wines in our CVA plots, but we can investigate the `$scores` data frame in the CVA results to see the raw observation scores, and use them to plot confidence ellipses.

```{r}
cva_res_oneway$scores %>% 
  as_tibble()

cva_res$scores %>%
  as_tibble()
```

We can add these to our mean-scores plots using the `ggplot2` layering capabilities--another example of using `ggplot2` over base R graphics.

```{r}
scores_p_1 + 
  stat_ellipse(data = cva_res_oneway$scores,
               mapping = aes(group = ProductName),
               type = "t") +
  coord_fixed()

# This is in some contrast to the original R Opus, in which HGH used 95% confidence circles

scores_p_1 +
  stat_ellipse(data = cva_res_oneway$scores,
               mapping = aes(group = ProductName),
               type = "euclid",
               level = 2 / sqrt(table(cva_res_oneway$scores[,1]))[1]) + 
  coord_fixed()
```

While I don't have immediate access to the original reference HGH cites, the calculation provided in the original **R Opus** and repeated above draws a circle of confidence based on the number of observations in each group, which to my mind seems likely to underestimate the overlap between samples.  On the other hand, my simple confidence ellipses are for the raw observations, rather than the "barycentric" (mean vector) confidence intervals that the method given by HGH hopes to provide.  We can look at this by looking at the actual observations plotted in the CVA space.

```{r}
scores_p_1 +
  geom_point(data = cva_res_oneway$scores,
             aes(color = ProductName),
             alpha = 1/3) +
  stat_ellipse(data = cva_res_oneway$scores,
               mapping = aes(fill = ProductName), 
               geom = "polygon",
               alpha = 0.1) +
  stat_ellipse(data = cva_res_oneway$scores,
               mapping = aes(group = ProductName, color = ProductName),
               type = "euclid",
               level = 2 / sqrt(table(cva_res_oneway$scores[,1]))[1]) +
  coord_fixed()
```

When we plot the actual observations, it is apparent that the proposed confidence circles don't do a good job of capturing the observed variability in the data.  They are probably overgenerous in their estimation of the separability of the individual observations, as from what I can tell the radius of the circles is simply a function of the number of observations.  This would decrease as the number of observations go up, which makes sense as more observations will give more stable mean vectors, but it doesn't really take advantage of the raw data. On the other hand, the confidence intervals based on raw observations (which are no longer barycentric) don't tell us much about mean separation--they are more akin to predicting the orientation of other new observations for each wine in the CVA space.

In the original **R Opus**, HGH provides a function from Helene Hopfer, Peter Buffon, and Vince Buffalo to calculate barycentric confidence ellipses based on, I believe, a similar method as that given in @peltierCanonical2015.  These functions rely on assumptions of bivariate normality for mean scores on the represented CVA axes, which is reasonable, but I think the function is quite opaque.  As noted in @peltierCanonical2015, a resampling approach could be used, so we're going to take this approach to illustrate such an approach.  Resampling, which involves perturbing the original data randomly and then examining its stability, is a nonparametric approach that is useful because it makes very few assumptions about either our data or analysis.

```{r}
# This is still going to be a bit complicated, but I will walk through the steps
# in my thinking.


# First, we will write two functions that just calculate the canonical variate
# scores for the means of our data.  These are based on the equations from
# Rencher 2002.
get_cva_1 <- function(means){
  
  cva_res_oneway$coeffs.raw[, 1] %*% means
  
}

get_cva_2 <- function(means){
  
  cva_res_oneway$coeffs.raw[, 2] %*% means
  
}

# Then, we write a convenience function that will resample our data with
# replacement, calculate centered means for our samples, and then project them
# into our original CVA space using the convenience functions we wrote above.
get_bootstrapped_means <- function(){
  
  descriptive_data %>%
  select(-NJ, -NR) %>%
  group_by(ProductName) %>%
    # Here we resample each wine - we draw 42 new observations for each wine
    # with replacement
  slice_sample(prop = 1, replace = TRUE) %>%
    # We calculate the mean for the newly drawn samples
  summarize_if(is.numeric, ~mean(.)) %>% 
    # And then we center the means (by subtracting the column means)
  mutate_if(is.numeric, ~. - mean(.)) %>%
    # Finally, we apply our projection functions (the linear combinations from
    # our CVA) to project the bootstrapped means into our new space
  nest(data = -ProductName) %>%
  mutate(means = map(data, ~as.matrix(.x) %>% t()),
         Can1 = map_dbl(means, ~get_cva_1(.x)),
         Can2 = map_dbl(means, ~get_cva_2(.x)),
         ) %>%
    select(ProductName, Can1, Can2)
  
}

# For example, here is the result of our function
get_bootstrapped_means()

# In order to generate a set of resampled confidence ellipses for our results,
# we'll run this 100 times for speed (for accuracy, 1000x would be better) and use
# stat_ellipse() to draw those.

bootstrapped_cva_means <- 
  tibble(boot_id = 1:100) %>%
  mutate(bootstrapped_data = map(boot_id, ~get_bootstrapped_means())) %>%
  unnest(bootstrapped_data)

scores_p_1 + 
  stat_ellipse(data = bootstrapped_cva_means,
               aes(fill = ProductName),
               geom = "polygon",
               alpha = 0.1) +
  coord_fixed() + 
  theme(legend.position = "none")
```

While this requires a bit of computation time (around 5 seconds on my M2 Mac Mini, writing in 2024), this is still pretty tractable.  It would take about 1 minute to do a "publishable" bootstrap of 1000 samples, which is still fine.  As we'll see, this resampling approach will generalize easily to other needs.

This is a more conservative result than that given by the bivariate normal ellipses in the original **R Opus**, although the actual conclusions of pairwise sample discrimination are identical.

## Packages used in this chapter

```{r}
sessionInfo()
```